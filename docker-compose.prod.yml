# Production Docker Compose
# Frontend is built as static files and served by nginx

services:
  # Backend API (FastAPI + Python)
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: mai-backend
    # ports:
    #   - "8000:8000"  # Only expose via nginx in production
    environment:
      - PYTHONUNBUFFERED=1
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - DATABASE_PATH=/app/data/checkpoints.db
      - LOG_LEVEL=INFO
      # Authentication
      - AUTH_USERS=admin:admin:admin;dev:dev:developer;user:user:user
      # LLM Provider (Ollama via OpenAI adapter)
      - LLM_PROVIDER=openai
      - OPENAI_BASE_URL=http://host.docker.internal:11434/v1
      - OPENAI_API_KEY=ollama
      - LLM_MODEL=gpt-oss:20b-cloud
    volumes:
      - ./backend:/app
      - ./backend/data:/app/data
      - backend_logs:/app/logs
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - mai-network
    restart: unless-stopped

  # Nginx + Frontend (static build)
  nginx:
    build:
      context: .
      dockerfile: nginx/Dockerfile.prod
    container_name: mai-nginx
    ports:
      - "80:80"
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - mai-network
    restart: unless-stopped

networks:
  mai-network:
    driver: bridge

volumes:
  backend_data:
    driver: local
  backend_logs:
    driver: local
