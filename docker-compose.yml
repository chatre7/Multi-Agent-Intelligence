services:
  # Ollama LLM Service
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: mai-ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   environment:
  #     - OLLAMA_HOST=0.0.0.0:11434
  #   healthcheck:
  #     test: [ "CMD", "curl", "-f", "http://localhost:11434/api/tags" ]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #   networks:
  #     - mai-network
  #   restart: unless-stopped

  # Backend API (FastAPI + Python)
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: mai-backend
    ports:
      - "8000:8000"
    # No exposed ports - accessed via nginx
    environment:
      - PYTHONUNBUFFERED=1
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - DATABASE_PATH=/app/data/checkpoints.db
      - LOG_LEVEL=INFO
      # Authentication
      - AUTH_MODE=jwt
      - AUTH_SECRET=dev-secret-key-change-in-production
      - AUTH_USERS=admin:admin:admin;dev:dev:developer;user:user:user
      # LLM Provider (Ollama via OpenAI adapter)
      - LLM_PROVIDER=openai
      - OPENAI_BASE_URL=http://host.docker.internal:11434/v1
      - OPENAI_API_KEY=ollama
    volumes:
      - ./backend:/app
      - backend_data:/app/data
      - backend_logs:/app/logs
    # depends_on:
    #   ollama:
    #     condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - mai-network
    restart: unless-stopped
    command: >
      sh -c "pip install -e . &&
             python -m uvicorn src.presentation.api.app:create_app --host 0.0.0.0 --port 8000 --reload"

  # Frontend (React + Vite)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: mai-frontend
    # No exposed ports - accessed via nginx
    environment:
      - VITE_API_BASE_URL=/api
      - VITE_WS_URL=/ws
      - BACKEND_HOST=backend
    volumes:
      - ./frontend:/app
      - /app/node_modules
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - mai-network
    restart: unless-stopped
    command: npm run dev -- --host 0.0.0.0

  # Nginx Reverse Proxy
  nginx:
    build:
      context: ./nginx
      dockerfile: Dockerfile
    container_name: mai-nginx
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      backend:
        condition: service_healthy
      frontend:
        condition: service_started
    networks:
      - mai-network
    restart: unless-stopped

networks:
  mai-network:
    driver: bridge

volumes:
  ollama_data:
    driver: local
  backend_data:
    driver: local
  backend_logs:
    driver: local
